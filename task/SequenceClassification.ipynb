{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "sys.path.insert(0, '../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pytorch-pretrained-bert in c:\\users\\user\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (0.6.2)\n",
      "Requirement already satisfied: torch>=0.4.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from pytorch-pretrained-bert) (2.0.1+cu118)\n",
      "Requirement already satisfied: numpy in c:\\users\\user\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from pytorch-pretrained-bert) (1.23.1)\n",
      "Requirement already satisfied: requests in c:\\users\\user\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from pytorch-pretrained-bert) (2.30.0)\n",
      "Requirement already satisfied: regex in c:\\users\\user\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from pytorch-pretrained-bert) (2023.5.5)\n",
      "Requirement already satisfied: boto3 in c:\\users\\user\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from pytorch-pretrained-bert) (1.34.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\user\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from pytorch-pretrained-bert) (4.65.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\user\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from torch>=0.4.1->pytorch-pretrained-bert) (1.11.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\user\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from torch>=0.4.1->pytorch-pretrained-bert) (3.1.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\user\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from torch>=0.4.1->pytorch-pretrained-bert) (3.12.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\user\\appdata\\roaming\\python\\python38\\site-packages (from torch>=0.4.1->pytorch-pretrained-bert) (4.5.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\user\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from torch>=0.4.1->pytorch-pretrained-bert) (3.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from requests->pytorch-pretrained-bert) (2.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\user\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from requests->pytorch-pretrained-bert) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from requests->pytorch-pretrained-bert) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from requests->pytorch-pretrained-bert) (2023.5.7)\n",
      "Requirement already satisfied: s3transfer<0.10.0,>=0.9.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from boto3->pytorch-pretrained-bert) (0.9.0)\n",
      "Requirement already satisfied: botocore<1.35.0,>=1.34.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from boto3->pytorch-pretrained-bert) (1.34.1)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from boto3->pytorch-pretrained-bert) (1.0.1)\n",
      "Requirement already satisfied: colorama; platform_system == \"Windows\" in c:\\users\\user\\appdata\\roaming\\python\\python38\\site-packages (from tqdm->pytorch-pretrained-bert) (0.4.6)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\user\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from sympy->torch>=0.4.1->pytorch-pretrained-bert) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from jinja2->torch>=0.4.1->pytorch-pretrained-bert) (2.1.2)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in c:\\users\\user\\appdata\\roaming\\python\\python38\\site-packages (from botocore<1.35.0,>=1.34.1->boto3->pytorch-pretrained-bert) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\user\\appdata\\roaming\\python\\python38\\site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.35.0,>=1.34.1->boto3->pytorch-pretrained-bert) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 20.2.1; however, version 23.3.1 is available.\n",
      "You should consider upgrading via the 'c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python38\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "pip install pytorch-pretrained-bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common.common import create_folder\n",
    "from common.pytorch import load_model\n",
    "import pytorch_pretrained_bert as Bert\n",
    "from model.utils import age_vocab\n",
    "from common.common import load_obj\n",
    "from dataLoader.NumericalTrain import NTLoader\n",
    "from torch.utils.data import DataLoader\n",
    "import pandas as pd\n",
    "from model.NT import BertForSequenceClassification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from model.optimiser import adam\n",
    "import sklearn.metrics as skm\n",
    "import numpy as np\n",
    "import torch\n",
    "import time\n",
    "import torch.nn as nn\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertConfig(Bert.modeling.BertConfig):\n",
    "    def __init__(self, config):\n",
    "        super(BertConfig, self).__init__(\n",
    "            vocab_size_or_config_json_file=config.get('vocab_size'),\n",
    "            hidden_size=config['hidden_size'],\n",
    "            num_hidden_layers=config.get('num_hidden_layers'),\n",
    "            num_attention_heads=config.get('num_attention_heads'),\n",
    "            intermediate_size=config.get('intermediate_size'),\n",
    "            hidden_act=config.get('hidden_act'),\n",
    "            hidden_dropout_prob=config.get('hidden_dropout_prob'),\n",
    "            attention_probs_dropout_prob=config.get('attention_probs_dropout_prob'),\n",
    "            max_position_embeddings = config.get('max_position_embedding'),\n",
    "            initializer_range=config.get('initializer_range'),\n",
    "        )\n",
    "        self.seg_vocab_size = config.get('seg_vocab_size')\n",
    "        self.age_vocab_size = config.get('age_vocab_size')\n",
    "        \n",
    "class TrainConfig(object):\n",
    "    def __init__(self, config):\n",
    "        self.batch_size = config.get('batch_size')\n",
    "        self.use_cuda = config.get('use_cuda')\n",
    "        self.max_len_seq = config.get('max_len_seq')\n",
    "        self.train_loader_workers = config.get('train_loader_workers')\n",
    "        self.test_loader_workers = config.get('test_loader_workers')\n",
    "        self.device = config.get('device')\n",
    "        self.output_dir = config.get('output_dir')\n",
    "        self.output_name = config.get('output_name')\n",
    "        self.best_name = config.get('best_name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_config = {\n",
    "    'data': '',  # formated data \n",
    "    'model_path': 'model', # where to save model\n",
    "    'model_name': 'SepsisBERT.csv', # model name\n",
    "    'file_name': '',  # log path\n",
    "}\n",
    "create_folder(file_config['model_path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_params = {\n",
    "    'max_seq_len': 280,\n",
    "    'max_age': 110,\n",
    "    'month': 1,\n",
    "    'age_symbol': None,\n",
    "    'min_visit': 5,\n",
    "    'gradient_accumulation_steps': 1\n",
    "}\n",
    "\n",
    "optim_param = {\n",
    "    'lr': 3e-5,\n",
    "    'warmup_proportion': 0.1,\n",
    "    'weight_decay': 0.01\n",
    "}\n",
    "\n",
    "train_params = {\n",
    "    'batch_size': 256,\n",
    "    'use_cuda': False,\n",
    "    'max_len_seq': global_params['max_seq_len'],\n",
    "    'device': 'cpu'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../preprocess/final_dataset/final_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HR</th>\n",
       "      <th>Temp</th>\n",
       "      <th>O2Sat</th>\n",
       "      <th>Resp</th>\n",
       "      <th>SepsisLabel</th>\n",
       "      <th>PatientID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{6: 101.0, 10: 87.0, 14: 81.0, 18: 87.0, 22: 9...</td>\n",
       "      <td>{6: 36.6, 10: 36.0, 14: 36.0, 18: 36.7, 22: 36...</td>\n",
       "      <td>{6: 100.0, 10: 100.0, 14: 100.0, 18: 100.0, 22...</td>\n",
       "      <td>{6: 20.0, 10: 20.0, 14: 20.0, 18: 20.0, 22: 18...</td>\n",
       "      <td>{6: 0, 10: 0, 14: 0, 18: 0, 22: 0, 26: 0, 30: ...</td>\n",
       "      <td>p116812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{0: 84.5, 1: 87.0, 2: 88.0, 3: 87.0, 4: 92.0, ...</td>\n",
       "      <td>{0: 34.3, 1: 34.6, 2: 35.5, 3: 36.2, 4: 36.65,...</td>\n",
       "      <td>{0: 96.0, 1: 96.0, 2: 95.0, 3: 98.0, 4: 98.5, ...</td>\n",
       "      <td>{0: 30.0, 1: 18.0, 2: 19.0, 3: 18.0, 4: 18.0, ...</td>\n",
       "      <td>{0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: ...</td>\n",
       "      <td>p109932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{0: 80.0, 1: 76.0, 2: 80.0, 3: 78.0, 4: 74.0, ...</td>\n",
       "      <td>{0: 36.5, 1: 36.25, 2: 36.25, 3: 36.1, 4: 36.0...</td>\n",
       "      <td>{0: 100.0, 1: 100.0, 2: 100.0, 3: 100.0, 4: 10...</td>\n",
       "      <td>{0: 13.5, 1: 12.0, 2: 12.0, 3: 12.0, 4: 12.5, ...</td>\n",
       "      <td>{0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: ...</td>\n",
       "      <td>p014977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{4: 83.0, 8: 81.0, 13: 85.0, 17: 92.0, 22: 87....</td>\n",
       "      <td>{4: 36.11, 8: 35.83, 13: 36.0, 17: 37.28, 22: ...</td>\n",
       "      <td>{4: 99.0, 8: 96.0, 13: 97.0, 17: 93.0, 22: 97....</td>\n",
       "      <td>{4: 16.0, 8: 17.0, 13: 19.0, 17: 17.0, 22: 15....</td>\n",
       "      <td>{4: 0, 8: 0, 13: 0, 17: 0, 22: 0, 28: 0, 39: 0}</td>\n",
       "      <td>p000902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{0: 102.5, 5: 100.0, 8: 102.0, 17: 108.0, 18: ...</td>\n",
       "      <td>{0: 36.89, 5: 37.06, 8: 36.17, 17: 36.17, 18: ...</td>\n",
       "      <td>{0: 96.0, 5: 98.0, 8: 95.0, 17: 89.0, 18: 100....</td>\n",
       "      <td>{0: 16.5, 5: 14.5, 8: 16.0, 17: 18.0, 18: 15.0...</td>\n",
       "      <td>{0: 0, 5: 0, 8: 0, 17: 0, 18: 0, 20: 0, 25: 0,...</td>\n",
       "      <td>p009098</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  HR   \n",
       "0  {6: 101.0, 10: 87.0, 14: 81.0, 18: 87.0, 22: 9...  \\\n",
       "1  {0: 84.5, 1: 87.0, 2: 88.0, 3: 87.0, 4: 92.0, ...   \n",
       "2  {0: 80.0, 1: 76.0, 2: 80.0, 3: 78.0, 4: 74.0, ...   \n",
       "3  {4: 83.0, 8: 81.0, 13: 85.0, 17: 92.0, 22: 87....   \n",
       "4  {0: 102.5, 5: 100.0, 8: 102.0, 17: 108.0, 18: ...   \n",
       "\n",
       "                                                Temp   \n",
       "0  {6: 36.6, 10: 36.0, 14: 36.0, 18: 36.7, 22: 36...  \\\n",
       "1  {0: 34.3, 1: 34.6, 2: 35.5, 3: 36.2, 4: 36.65,...   \n",
       "2  {0: 36.5, 1: 36.25, 2: 36.25, 3: 36.1, 4: 36.0...   \n",
       "3  {4: 36.11, 8: 35.83, 13: 36.0, 17: 37.28, 22: ...   \n",
       "4  {0: 36.89, 5: 37.06, 8: 36.17, 17: 36.17, 18: ...   \n",
       "\n",
       "                                               O2Sat   \n",
       "0  {6: 100.0, 10: 100.0, 14: 100.0, 18: 100.0, 22...  \\\n",
       "1  {0: 96.0, 1: 96.0, 2: 95.0, 3: 98.0, 4: 98.5, ...   \n",
       "2  {0: 100.0, 1: 100.0, 2: 100.0, 3: 100.0, 4: 10...   \n",
       "3  {4: 99.0, 8: 96.0, 13: 97.0, 17: 93.0, 22: 97....   \n",
       "4  {0: 96.0, 5: 98.0, 8: 95.0, 17: 89.0, 18: 100....   \n",
       "\n",
       "                                                Resp   \n",
       "0  {6: 20.0, 10: 20.0, 14: 20.0, 18: 20.0, 22: 18...  \\\n",
       "1  {0: 30.0, 1: 18.0, 2: 19.0, 3: 18.0, 4: 18.0, ...   \n",
       "2  {0: 13.5, 1: 12.0, 2: 12.0, 3: 12.0, 4: 12.5, ...   \n",
       "3  {4: 16.0, 8: 17.0, 13: 19.0, 17: 17.0, 22: 15....   \n",
       "4  {0: 16.5, 5: 14.5, 8: 16.0, 17: 18.0, 18: 15.0...   \n",
       "\n",
       "                                         SepsisLabel PatientID  \n",
       "0  {6: 0, 10: 0, 14: 0, 18: 0, 22: 0, 26: 0, 30: ...   p116812  \n",
       "1  {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: ...   p109932  \n",
       "2  {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: ...   p014977  \n",
       "3    {4: 0, 8: 0, 13: 0, 17: 0, 22: 0, 28: 0, 39: 0}   p000902  \n",
       "4  {0: 0, 5: 0, 8: 0, 17: 0, 18: 0, 20: 0, 25: 0,...   p009098  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the DataFrame into training and testing sets\n",
    "train_df, test_df = train_test_split(data, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the NTLoader for Training\n",
    "Dset = NTLoader(train_df, max_len=global_params['max_seq_len'])\n",
    "trainload = DataLoader(dataset=Dset, batch_size=train_params['batch_size'], shuffle=True, num_workers=3)\n",
    "\n",
    "# Initialize the NTLoader for Testing\n",
    "Dset = NTLoader(test_df, max_len=global_params['max_seq_len'])\n",
    "testload = DataLoader(dataset=Dset, batch_size=train_params['batch_size'], shuffle=True, num_workers=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = {\n",
    "    'vocab_size': 10, # number of disease + symbols for word embedding\n",
    "    'hidden_size': 288, # word embedding and seg embedding hidden size\n",
    "    'max_position_embedding': train_params['max_len_seq'], # maximum number of tokens\n",
    "    'hidden_dropout_prob': 0.1, # dropout rate\n",
    "    'num_hidden_layers': 6, # number of multi-head attention layers required\n",
    "    'num_attention_heads': 12, # number of attention heads\n",
    "    'attention_probs_dropout_prob': 0.1, # multi-head attention dropout rate\n",
    "    'intermediate_size': 512, # the size of the \"intermediate\" layer in the transformer encoder\n",
    "    'hidden_act': 'gelu', # The non-linear activation function in the encoder and the pooler \"gelu\", 'relu', 'swish' are supported\n",
    "    'initializer_range': 0.02, # parameter weight initializer range\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Configuration\n",
    "config = BertConfig(model_config)\n",
    "\n",
    "# Initialize the model\n",
    "num_features = 4  # Number of input features\n",
    "model = BertForSequenceClassification(config, num_features=num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t_total value of -1 results in schedule not being applied\n"
     ]
    }
   ],
   "source": [
    "#model = model.to(train_params['device'])\n",
    "optim = adam(params=list(model.named_parameters()), config=optim_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_acc(label, pred):\n",
    "    logs = nn.LogSoftmax()\n",
    "    label=label.cpu().numpy()\n",
    "    ind = np.where(label!=-1)[0]\n",
    "    truepred = pred.detach().cpu().numpy()\n",
    "    truepred = truepred[ind]\n",
    "    truelabel = label[ind]\n",
    "    truepred = logs(torch.tensor(truepred))\n",
    "    outs = [np.argmax(pred_x) for pred_x in truepred.numpy()]\n",
    "    precision = skm.precision_score(truelabel, outs, average='micro')\n",
    "    return precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "\n",
    "def train(epoch, loader, model, device, optimizer, print_step=256):\n",
    "    model.train()\n",
    "    tr_loss = 0\n",
    "    temp_loss = 0\n",
    "    start = time.time()\n",
    "\n",
    "    for step, batch in enumerate(loader):\n",
    "        # Move batch to the specified device\n",
    "        features, labels = batch[0].to(device), batch[1].to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss, logits = model(features, labels=labels)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Logging\n",
    "        temp_loss += loss.item()\n",
    "        tr_loss += loss.item()\n",
    "\n",
    "        elapsed = time.time() - start\n",
    "        print(f\"Epoch: {epoch} | Step: {step} | Avg Loss: {temp_loss / print_step:.4f} | Time Elapsed: {elapsed:.2f}s\")\n",
    "        temp_loss = 0\n",
    "        start = time.time()\n",
    "\n",
    "    # Calculate total loss for the epoch\n",
    "    total_loss = tr_loss / len(loader)\n",
    "\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Step: 0 | Avg Loss: 0.0022 | Time Elapsed: 21.34s\n",
      "Epoch: 0 | Step: 1 | Avg Loss: 0.0000 | Time Elapsed: 8.02s\n",
      "Epoch: 0 | Step: 2 | Avg Loss: 0.0000 | Time Elapsed: 8.12s\n",
      "Epoch: 0 | Step: 3 | Avg Loss: 0.0000 | Time Elapsed: 7.63s\n",
      "Epoch: 0 | Step: 4 | Avg Loss: 0.0000 | Time Elapsed: 7.77s\n",
      "Epoch: 0 | Step: 5 | Avg Loss: 0.0000 | Time Elapsed: 8.94s\n",
      "Epoch: 0 | Step: 6 | Avg Loss: 0.0000 | Time Elapsed: 9.30s\n",
      "Epoch: 0 | Step: 7 | Avg Loss: 0.0000 | Time Elapsed: 8.75s\n",
      "Epoch: 0 | Step: 8 | Avg Loss: 0.0000 | Time Elapsed: 7.58s\n",
      "Epoch: 0 | Step: 9 | Avg Loss: 0.0000 | Time Elapsed: 7.78s\n",
      "Epoch: 0 | Step: 10 | Avg Loss: 0.0000 | Time Elapsed: 8.07s\n",
      "Epoch: 0 | Step: 11 | Avg Loss: 0.0000 | Time Elapsed: 7.67s\n",
      "Epoch: 0 | Step: 12 | Avg Loss: 0.0000 | Time Elapsed: 7.70s\n",
      "Epoch: 0 | Step: 13 | Avg Loss: 0.0000 | Time Elapsed: 8.02s\n",
      "Epoch: 0 | Step: 14 | Avg Loss: 0.0000 | Time Elapsed: 7.62s\n",
      "Epoch: 0 | Step: 15 | Avg Loss: 0.0000 | Time Elapsed: 7.61s\n",
      "Epoch: 0 | Step: 16 | Avg Loss: 0.0000 | Time Elapsed: 7.60s\n",
      "Epoch: 0 | Step: 17 | Avg Loss: 0.0000 | Time Elapsed: 7.65s\n",
      "Epoch: 0 | Step: 18 | Avg Loss: 0.0000 | Time Elapsed: 8.20s\n",
      "Epoch: 0 | Step: 19 | Avg Loss: 0.0000 | Time Elapsed: 7.67s\n",
      "Epoch: 0 | Step: 20 | Avg Loss: 0.0000 | Time Elapsed: 7.70s\n",
      "Epoch: 0 | Step: 21 | Avg Loss: 0.0000 | Time Elapsed: 7.84s\n",
      "Epoch: 0 | Step: 22 | Avg Loss: 0.0000 | Time Elapsed: 7.79s\n",
      "Epoch: 0 | Step: 23 | Avg Loss: 0.0000 | Time Elapsed: 7.62s\n",
      "Epoch: 0 | Step: 24 | Avg Loss: 0.0000 | Time Elapsed: 7.64s\n",
      "Epoch: 0 | Step: 25 | Avg Loss: 0.0000 | Time Elapsed: 7.72s\n",
      "Epoch: 0 | Step: 26 | Avg Loss: 0.0000 | Time Elapsed: 7.89s\n",
      "Epoch: 0 | Step: 27 | Avg Loss: 0.0000 | Time Elapsed: 7.92s\n",
      "Epoch: 0 | Step: 28 | Avg Loss: 0.0000 | Time Elapsed: 7.69s\n",
      "Epoch: 0 | Step: 29 | Avg Loss: 0.0000 | Time Elapsed: 7.67s\n",
      "Epoch: 0 | Step: 30 | Avg Loss: 0.0000 | Time Elapsed: 7.95s\n",
      "Epoch: 0 | Step: 31 | Avg Loss: 0.0000 | Time Elapsed: 7.76s\n",
      "Epoch: 0 | Step: 32 | Avg Loss: 0.0000 | Time Elapsed: 7.68s\n",
      "Epoch: 0 | Step: 33 | Avg Loss: 0.0000 | Time Elapsed: 7.72s\n",
      "Epoch: 0 | Step: 34 | Avg Loss: 0.0000 | Time Elapsed: 7.66s\n",
      "Epoch: 0 | Step: 35 | Avg Loss: 0.0000 | Time Elapsed: 7.90s\n",
      "Epoch: 0 | Step: 36 | Avg Loss: 0.0000 | Time Elapsed: 7.60s\n",
      "Epoch: 0 | Step: 37 | Avg Loss: 0.0000 | Time Elapsed: 7.75s\n",
      "Epoch: 0 | Step: 38 | Avg Loss: 0.0000 | Time Elapsed: 7.94s\n",
      "Epoch: 0 | Step: 39 | Avg Loss: 0.0000 | Time Elapsed: 7.98s\n",
      "Epoch: 0 | Step: 40 | Avg Loss: 0.0000 | Time Elapsed: 8.06s\n",
      "Epoch: 0 | Step: 41 | Avg Loss: 0.0000 | Time Elapsed: 8.22s\n",
      "Epoch: 0 | Step: 42 | Avg Loss: 0.0000 | Time Elapsed: 7.80s\n",
      "Epoch: 0 | Step: 43 | Avg Loss: 0.0000 | Time Elapsed: 7.89s\n",
      "Epoch: 0 | Step: 44 | Avg Loss: 0.0000 | Time Elapsed: 7.80s\n",
      "Epoch: 0 | Step: 45 | Avg Loss: 0.0000 | Time Elapsed: 7.64s\n",
      "Epoch: 0 | Step: 46 | Avg Loss: 0.0000 | Time Elapsed: 8.10s\n",
      "Epoch: 0 | Step: 47 | Avg Loss: 0.0000 | Time Elapsed: 7.85s\n",
      "Epoch: 0 | Step: 48 | Avg Loss: 0.0000 | Time Elapsed: 7.85s\n",
      "Epoch: 0 | Step: 49 | Avg Loss: 0.0000 | Time Elapsed: 7.76s\n",
      "Epoch: 0 | Step: 50 | Avg Loss: 0.0000 | Time Elapsed: 7.70s\n",
      "Epoch: 0 | Step: 51 | Avg Loss: 0.0000 | Time Elapsed: 7.70s\n",
      "Epoch: 0 | Step: 52 | Avg Loss: 0.0000 | Time Elapsed: 7.96s\n",
      "Epoch: 0 | Step: 53 | Avg Loss: 0.0000 | Time Elapsed: 7.69s\n",
      "Epoch: 0 | Step: 54 | Avg Loss: 0.0000 | Time Elapsed: 7.74s\n",
      "Epoch: 0 | Step: 55 | Avg Loss: 0.0000 | Time Elapsed: 7.84s\n",
      "Epoch: 0 | Step: 56 | Avg Loss: 0.0000 | Time Elapsed: 7.75s\n",
      "Epoch: 0 | Step: 57 | Avg Loss: 0.0000 | Time Elapsed: 7.75s\n",
      "Epoch: 0 | Step: 58 | Avg Loss: 0.0000 | Time Elapsed: 7.80s\n",
      "Epoch: 0 | Step: 59 | Avg Loss: 0.0000 | Time Elapsed: 7.61s\n",
      "Epoch: 0 | Step: 60 | Avg Loss: 0.0000 | Time Elapsed: 7.85s\n",
      "Epoch: 0 | Step: 61 | Avg Loss: 0.0000 | Time Elapsed: 7.85s\n",
      "Epoch: 0 | Step: 62 | Avg Loss: 0.0000 | Time Elapsed: 7.68s\n",
      "Epoch: 0 | Step: 63 | Avg Loss: 0.0000 | Time Elapsed: 7.74s\n",
      "Epoch: 0 | Step: 64 | Avg Loss: 0.0000 | Time Elapsed: 7.97s\n",
      "Epoch: 0 | Step: 65 | Avg Loss: 0.0000 | Time Elapsed: 7.66s\n",
      "Epoch: 0 | Step: 66 | Avg Loss: 0.0000 | Time Elapsed: 7.75s\n",
      "Epoch: 0 | Step: 67 | Avg Loss: 0.0000 | Time Elapsed: 7.67s\n",
      "Epoch: 0 | Step: 68 | Avg Loss: 0.0000 | Time Elapsed: 7.76s\n",
      "Epoch: 0 | Step: 69 | Avg Loss: 0.0000 | Time Elapsed: 7.97s\n",
      "Epoch: 0 | Step: 70 | Avg Loss: 0.0000 | Time Elapsed: 7.63s\n",
      "Epoch: 0 | Step: 71 | Avg Loss: 0.0000 | Time Elapsed: 7.68s\n",
      "Epoch: 0 | Step: 72 | Avg Loss: 0.0000 | Time Elapsed: 8.08s\n",
      "Epoch: 0 | Step: 73 | Avg Loss: 0.0000 | Time Elapsed: 7.68s\n",
      "Epoch: 0 | Step: 74 | Avg Loss: 0.0000 | Time Elapsed: 7.64s\n",
      "Epoch: 0 | Step: 75 | Avg Loss: 0.0000 | Time Elapsed: 7.74s\n",
      "Epoch: 0 | Step: 76 | Avg Loss: 0.0000 | Time Elapsed: 7.66s\n",
      "Epoch: 0 | Step: 77 | Avg Loss: 0.0000 | Time Elapsed: 7.84s\n",
      "Epoch: 0 | Step: 78 | Avg Loss: 0.0000 | Time Elapsed: 7.73s\n",
      "Epoch: 0 | Step: 79 | Avg Loss: 0.0000 | Time Elapsed: 7.65s\n",
      "Epoch: 0 | Step: 80 | Avg Loss: 0.0000 | Time Elapsed: 7.86s\n",
      "Epoch: 0 | Step: 81 | Avg Loss: 0.0000 | Time Elapsed: 7.87s\n",
      "Epoch: 0 | Step: 82 | Avg Loss: 0.0000 | Time Elapsed: 7.70s\n",
      "Epoch: 0 | Step: 83 | Avg Loss: 0.0000 | Time Elapsed: 7.62s\n",
      "Epoch: 0 | Step: 84 | Avg Loss: 0.0000 | Time Elapsed: 7.60s\n",
      "Epoch: 0 | Step: 85 | Avg Loss: 0.0000 | Time Elapsed: 7.60s\n",
      "Epoch: 0 | Step: 86 | Avg Loss: 0.0000 | Time Elapsed: 7.95s\n",
      "Epoch: 0 | Step: 87 | Avg Loss: 0.0000 | Time Elapsed: 7.63s\n",
      "Epoch: 0 | Step: 88 | Avg Loss: 0.0000 | Time Elapsed: 7.72s\n",
      "Epoch: 0 | Step: 89 | Avg Loss: 0.0000 | Time Elapsed: 8.02s\n",
      "Epoch: 0 | Step: 90 | Avg Loss: 0.0000 | Time Elapsed: 7.70s\n",
      "Epoch: 0 | Step: 91 | Avg Loss: 0.0000 | Time Elapsed: 7.62s\n",
      "Epoch: 0 | Step: 92 | Avg Loss: 0.0000 | Time Elapsed: 7.67s\n",
      "Epoch: 0 | Step: 93 | Avg Loss: 0.0000 | Time Elapsed: 7.60s\n",
      "Epoch: 0 | Step: 94 | Avg Loss: 0.0000 | Time Elapsed: 7.75s\n",
      "Epoch: 0 | Step: 95 | Avg Loss: 0.0000 | Time Elapsed: 7.77s\n",
      "Epoch: 0 | Step: 96 | Avg Loss: 0.0000 | Time Elapsed: 7.76s\n",
      "Epoch: 0 | Step: 97 | Avg Loss: 0.0000 | Time Elapsed: 7.75s\n",
      "Epoch: 0 | Step: 98 | Avg Loss: 0.0000 | Time Elapsed: 7.86s\n",
      "Epoch: 0 | Step: 99 | Avg Loss: 0.0000 | Time Elapsed: 7.60s\n",
      "Epoch: 0 | Step: 100 | Avg Loss: 0.0000 | Time Elapsed: 7.67s\n",
      "Epoch: 0 | Step: 101 | Avg Loss: 0.0000 | Time Elapsed: 7.62s\n",
      "Epoch: 0 | Step: 102 | Avg Loss: 0.0000 | Time Elapsed: 7.68s\n",
      "Epoch: 0 | Step: 103 | Avg Loss: 0.0000 | Time Elapsed: 7.88s\n",
      "Epoch: 0 | Step: 104 | Avg Loss: 0.0000 | Time Elapsed: 7.87s\n",
      "Epoch: 0 | Step: 105 | Avg Loss: 0.0000 | Time Elapsed: 7.70s\n",
      "Epoch: 0 | Step: 106 | Avg Loss: 0.0000 | Time Elapsed: 7.83s\n",
      "Epoch: 0 | Step: 107 | Avg Loss: 0.0000 | Time Elapsed: 7.82s\n",
      "Epoch: 0 | Step: 108 | Avg Loss: 0.0000 | Time Elapsed: 7.63s\n",
      "Epoch: 0 | Step: 109 | Avg Loss: 0.0000 | Time Elapsed: 7.64s\n",
      "Epoch: 0 | Step: 110 | Avg Loss: 0.0000 | Time Elapsed: 7.65s\n",
      "Epoch: 0 | Step: 111 | Avg Loss: 0.0000 | Time Elapsed: 7.87s\n",
      "Epoch: 0 | Step: 112 | Avg Loss: 0.0000 | Time Elapsed: 7.86s\n",
      "Epoch: 0 | Step: 113 | Avg Loss: 0.0000 | Time Elapsed: 7.56s\n",
      "Epoch: 0 | Step: 114 | Avg Loss: 0.0000 | Time Elapsed: 7.69s\n",
      "Epoch: 0 | Step: 115 | Avg Loss: 0.0000 | Time Elapsed: 7.95s\n",
      "Epoch: 0 | Step: 116 | Avg Loss: 0.0000 | Time Elapsed: 7.70s\n",
      "Epoch: 0 | Step: 117 | Avg Loss: 0.0000 | Time Elapsed: 7.65s\n",
      "Epoch: 0 | Step: 118 | Avg Loss: 0.0000 | Time Elapsed: 7.71s\n",
      "Epoch: 0 | Step: 119 | Avg Loss: 0.0000 | Time Elapsed: 7.80s\n",
      "Epoch: 0 | Step: 120 | Avg Loss: 0.0000 | Time Elapsed: 8.05s\n",
      "Epoch: 0 | Step: 121 | Avg Loss: 0.0000 | Time Elapsed: 7.70s\n",
      "Epoch: 0 | Step: 122 | Avg Loss: 0.0000 | Time Elapsed: 7.61s\n",
      "Epoch: 0 | Step: 123 | Avg Loss: 0.0000 | Time Elapsed: 7.83s\n",
      "Epoch: 0 | Step: 124 | Avg Loss: 0.0000 | Time Elapsed: 4.74s\n",
      "Epoch 0 finished with loss: 0.0047\n",
      "Epoch: 1 | Step: 0 | Avg Loss: 0.0000 | Time Elapsed: 15.87s\n",
      "Epoch: 1 | Step: 1 | Avg Loss: 0.0000 | Time Elapsed: 7.89s\n",
      "Epoch: 1 | Step: 2 | Avg Loss: 0.0000 | Time Elapsed: 7.69s\n",
      "Epoch: 1 | Step: 3 | Avg Loss: 0.0000 | Time Elapsed: 7.95s\n",
      "Epoch: 1 | Step: 4 | Avg Loss: 0.0000 | Time Elapsed: 7.68s\n",
      "Epoch: 1 | Step: 5 | Avg Loss: 0.0000 | Time Elapsed: 7.68s\n",
      "Epoch: 1 | Step: 6 | Avg Loss: 0.0000 | Time Elapsed: 7.99s\n",
      "Epoch: 1 | Step: 7 | Avg Loss: 0.0000 | Time Elapsed: 7.75s\n",
      "Epoch: 1 | Step: 8 | Avg Loss: 0.0000 | Time Elapsed: 7.67s\n",
      "Epoch: 1 | Step: 9 | Avg Loss: 0.0000 | Time Elapsed: 7.79s\n",
      "Epoch: 1 | Step: 10 | Avg Loss: 0.0000 | Time Elapsed: 7.72s\n",
      "Epoch: 1 | Step: 11 | Avg Loss: 0.0000 | Time Elapsed: 7.89s\n",
      "Epoch: 1 | Step: 12 | Avg Loss: 0.0000 | Time Elapsed: 7.70s\n",
      "Epoch: 1 | Step: 13 | Avg Loss: 0.0000 | Time Elapsed: 7.66s\n",
      "Epoch: 1 | Step: 14 | Avg Loss: 0.0000 | Time Elapsed: 7.80s\n",
      "Epoch: 1 | Step: 15 | Avg Loss: 0.0000 | Time Elapsed: 7.74s\n",
      "Epoch: 1 | Step: 16 | Avg Loss: 0.0000 | Time Elapsed: 7.59s\n",
      "Epoch: 1 | Step: 17 | Avg Loss: 0.0000 | Time Elapsed: 7.78s\n",
      "Epoch: 1 | Step: 18 | Avg Loss: 0.0000 | Time Elapsed: 7.63s\n",
      "Epoch: 1 | Step: 19 | Avg Loss: 0.0000 | Time Elapsed: 7.67s\n",
      "Epoch: 1 | Step: 20 | Avg Loss: 0.0000 | Time Elapsed: 7.84s\n",
      "Epoch: 1 | Step: 21 | Avg Loss: 0.0000 | Time Elapsed: 7.67s\n",
      "Epoch: 1 | Step: 22 | Avg Loss: 0.0000 | Time Elapsed: 7.64s\n",
      "Epoch: 1 | Step: 23 | Avg Loss: 0.0000 | Time Elapsed: 7.97s\n",
      "Epoch: 1 | Step: 24 | Avg Loss: 0.0000 | Time Elapsed: 7.64s\n",
      "Epoch: 1 | Step: 25 | Avg Loss: 0.0000 | Time Elapsed: 7.81s\n",
      "Epoch: 1 | Step: 26 | Avg Loss: 0.0000 | Time Elapsed: 7.60s\n",
      "Epoch: 1 | Step: 27 | Avg Loss: 0.0000 | Time Elapsed: 7.59s\n",
      "Epoch: 1 | Step: 28 | Avg Loss: 0.0000 | Time Elapsed: 7.76s\n",
      "Epoch: 1 | Step: 29 | Avg Loss: 0.0000 | Time Elapsed: 7.83s\n",
      "Epoch: 1 | Step: 30 | Avg Loss: 0.0000 | Time Elapsed: 7.61s\n",
      "Epoch: 1 | Step: 31 | Avg Loss: 0.0000 | Time Elapsed: 7.63s\n",
      "Epoch: 1 | Step: 32 | Avg Loss: 0.0000 | Time Elapsed: 8.13s\n",
      "Epoch: 1 | Step: 33 | Avg Loss: 0.0000 | Time Elapsed: 7.84s\n",
      "Epoch: 1 | Step: 34 | Avg Loss: 0.0000 | Time Elapsed: 7.72s\n",
      "Epoch: 1 | Step: 35 | Avg Loss: 0.0000 | Time Elapsed: 7.57s\n",
      "Epoch: 1 | Step: 36 | Avg Loss: 0.0000 | Time Elapsed: 7.66s\n",
      "Epoch: 1 | Step: 37 | Avg Loss: 0.0000 | Time Elapsed: 7.96s\n",
      "Epoch: 1 | Step: 38 | Avg Loss: 0.0000 | Time Elapsed: 7.70s\n",
      "Epoch: 1 | Step: 39 | Avg Loss: 0.0000 | Time Elapsed: 7.73s\n",
      "Epoch: 1 | Step: 40 | Avg Loss: 0.0000 | Time Elapsed: 7.99s\n",
      "Epoch: 1 | Step: 41 | Avg Loss: 0.0000 | Time Elapsed: 7.56s\n",
      "Epoch: 1 | Step: 42 | Avg Loss: 0.0000 | Time Elapsed: 7.61s\n",
      "Epoch: 1 | Step: 43 | Avg Loss: 0.0000 | Time Elapsed: 7.72s\n",
      "Epoch: 1 | Step: 44 | Avg Loss: 0.0000 | Time Elapsed: 7.68s\n",
      "Epoch: 1 | Step: 45 | Avg Loss: 0.0000 | Time Elapsed: 7.78s\n",
      "Epoch: 1 | Step: 46 | Avg Loss: 0.0000 | Time Elapsed: 7.86s\n",
      "Epoch: 1 | Step: 47 | Avg Loss: 0.0000 | Time Elapsed: 7.68s\n",
      "Epoch: 1 | Step: 48 | Avg Loss: 0.0000 | Time Elapsed: 7.80s\n",
      "Epoch: 1 | Step: 49 | Avg Loss: 0.0000 | Time Elapsed: 7.98s\n",
      "Epoch: 1 | Step: 50 | Avg Loss: 0.0000 | Time Elapsed: 7.66s\n",
      "Epoch: 1 | Step: 51 | Avg Loss: 0.0000 | Time Elapsed: 7.65s\n",
      "Epoch: 1 | Step: 52 | Avg Loss: 0.0000 | Time Elapsed: 7.61s\n",
      "Epoch: 1 | Step: 53 | Avg Loss: 0.0000 | Time Elapsed: 7.65s\n",
      "Epoch: 1 | Step: 54 | Avg Loss: 0.0000 | Time Elapsed: 7.93s\n",
      "Epoch: 1 | Step: 55 | Avg Loss: 0.0000 | Time Elapsed: 7.61s\n",
      "Epoch: 1 | Step: 56 | Avg Loss: 0.0000 | Time Elapsed: 7.88s\n",
      "Epoch: 1 | Step: 57 | Avg Loss: 0.0000 | Time Elapsed: 7.96s\n",
      "Epoch: 1 | Step: 58 | Avg Loss: 0.0000 | Time Elapsed: 7.74s\n",
      "Epoch: 1 | Step: 59 | Avg Loss: 0.0000 | Time Elapsed: 7.66s\n",
      "Epoch: 1 | Step: 60 | Avg Loss: 0.0000 | Time Elapsed: 7.62s\n",
      "Epoch: 1 | Step: 61 | Avg Loss: 0.0000 | Time Elapsed: 7.67s\n",
      "Epoch: 1 | Step: 62 | Avg Loss: 0.0000 | Time Elapsed: 7.75s\n",
      "Epoch: 1 | Step: 63 | Avg Loss: 0.0000 | Time Elapsed: 7.81s\n",
      "Epoch: 1 | Step: 64 | Avg Loss: 0.0000 | Time Elapsed: 7.87s\n",
      "Epoch: 1 | Step: 65 | Avg Loss: 0.0000 | Time Elapsed: 7.54s\n",
      "Epoch: 1 | Step: 66 | Avg Loss: 0.0000 | Time Elapsed: 7.93s\n",
      "Epoch: 1 | Step: 67 | Avg Loss: 0.0000 | Time Elapsed: 7.63s\n",
      "Epoch: 1 | Step: 68 | Avg Loss: 0.0000 | Time Elapsed: 7.74s\n",
      "Epoch: 1 | Step: 69 | Avg Loss: 0.0000 | Time Elapsed: 7.65s\n",
      "Epoch: 1 | Step: 70 | Avg Loss: 0.0000 | Time Elapsed: 7.63s\n",
      "Epoch: 1 | Step: 71 | Avg Loss: 0.0000 | Time Elapsed: 7.98s\n",
      "Epoch: 1 | Step: 72 | Avg Loss: 0.0000 | Time Elapsed: 7.85s\n",
      "Epoch: 1 | Step: 73 | Avg Loss: 0.0000 | Time Elapsed: 7.67s\n",
      "Epoch: 1 | Step: 74 | Avg Loss: 0.0000 | Time Elapsed: 7.75s\n",
      "Epoch: 1 | Step: 75 | Avg Loss: 0.0000 | Time Elapsed: 7.71s\n",
      "Epoch: 1 | Step: 76 | Avg Loss: 0.0000 | Time Elapsed: 7.72s\n",
      "Epoch: 1 | Step: 77 | Avg Loss: 0.0000 | Time Elapsed: 7.65s\n",
      "Epoch: 1 | Step: 78 | Avg Loss: 0.0000 | Time Elapsed: 7.65s\n",
      "Epoch: 1 | Step: 79 | Avg Loss: 0.0000 | Time Elapsed: 7.75s\n",
      "Epoch: 1 | Step: 80 | Avg Loss: 0.0000 | Time Elapsed: 8.04s\n",
      "Epoch: 1 | Step: 81 | Avg Loss: 0.0000 | Time Elapsed: 7.68s\n",
      "Epoch: 1 | Step: 82 | Avg Loss: 0.0000 | Time Elapsed: 7.71s\n",
      "Epoch: 1 | Step: 83 | Avg Loss: 0.0000 | Time Elapsed: 7.93s\n",
      "Epoch: 1 | Step: 84 | Avg Loss: 0.0000 | Time Elapsed: 7.58s\n",
      "Epoch: 1 | Step: 85 | Avg Loss: 0.0000 | Time Elapsed: 7.62s\n",
      "Epoch: 1 | Step: 86 | Avg Loss: 0.0000 | Time Elapsed: 7.67s\n",
      "Epoch: 1 | Step: 87 | Avg Loss: 0.0000 | Time Elapsed: 7.83s\n",
      "Epoch: 1 | Step: 88 | Avg Loss: 0.0000 | Time Elapsed: 7.93s\n",
      "Epoch: 1 | Step: 89 | Avg Loss: 0.0000 | Time Elapsed: 7.64s\n",
      "Epoch: 1 | Step: 90 | Avg Loss: 0.0000 | Time Elapsed: 7.67s\n",
      "Epoch: 1 | Step: 91 | Avg Loss: 0.0000 | Time Elapsed: 7.82s\n",
      "Epoch: 1 | Step: 92 | Avg Loss: 0.0000 | Time Elapsed: 7.80s\n",
      "Epoch: 1 | Step: 93 | Avg Loss: 0.0000 | Time Elapsed: 7.75s\n",
      "Epoch: 1 | Step: 94 | Avg Loss: 0.0000 | Time Elapsed: 7.62s\n",
      "Epoch: 1 | Step: 95 | Avg Loss: 0.0000 | Time Elapsed: 7.92s\n",
      "Epoch: 1 | Step: 96 | Avg Loss: 0.0000 | Time Elapsed: 7.70s\n",
      "Epoch: 1 | Step: 97 | Avg Loss: 0.0000 | Time Elapsed: 7.92s\n",
      "Epoch: 1 | Step: 98 | Avg Loss: 0.0000 | Time Elapsed: 7.60s\n",
      "Epoch: 1 | Step: 99 | Avg Loss: 0.0000 | Time Elapsed: 7.62s\n",
      "Epoch: 1 | Step: 100 | Avg Loss: 0.0000 | Time Elapsed: 7.87s\n",
      "Epoch: 1 | Step: 101 | Avg Loss: 0.0000 | Time Elapsed: 7.72s\n",
      "Epoch: 1 | Step: 102 | Avg Loss: 0.0000 | Time Elapsed: 7.71s\n",
      "Epoch: 1 | Step: 103 | Avg Loss: 0.0000 | Time Elapsed: 7.74s\n",
      "Epoch: 1 | Step: 104 | Avg Loss: 0.0000 | Time Elapsed: 7.59s\n",
      "Epoch: 1 | Step: 105 | Avg Loss: 0.0000 | Time Elapsed: 7.82s\n",
      "Epoch: 1 | Step: 106 | Avg Loss: 0.0000 | Time Elapsed: 7.74s\n",
      "Epoch: 1 | Step: 107 | Avg Loss: 0.0000 | Time Elapsed: 7.77s\n",
      "Epoch: 1 | Step: 108 | Avg Loss: 0.0000 | Time Elapsed: 7.77s\n",
      "Epoch: 1 | Step: 109 | Avg Loss: 0.0000 | Time Elapsed: 7.83s\n",
      "Epoch: 1 | Step: 110 | Avg Loss: 0.0000 | Time Elapsed: 7.67s\n",
      "Epoch: 1 | Step: 111 | Avg Loss: 0.0000 | Time Elapsed: 7.78s\n",
      "Epoch: 1 | Step: 112 | Avg Loss: 0.0000 | Time Elapsed: 7.67s\n",
      "Epoch: 1 | Step: 113 | Avg Loss: 0.0000 | Time Elapsed: 7.60s\n",
      "Epoch: 1 | Step: 114 | Avg Loss: 0.0000 | Time Elapsed: 7.97s\n",
      "Epoch: 1 | Step: 115 | Avg Loss: 0.0000 | Time Elapsed: 7.80s\n",
      "Epoch: 1 | Step: 116 | Avg Loss: 0.0000 | Time Elapsed: 7.66s\n",
      "Epoch: 1 | Step: 117 | Avg Loss: 0.0000 | Time Elapsed: 7.92s\n",
      "Epoch: 1 | Step: 118 | Avg Loss: 0.0000 | Time Elapsed: 7.62s\n",
      "Epoch: 1 | Step: 119 | Avg Loss: 0.0000 | Time Elapsed: 7.76s\n",
      "Epoch: 1 | Step: 120 | Avg Loss: 0.0000 | Time Elapsed: 7.58s\n",
      "Epoch: 1 | Step: 121 | Avg Loss: 0.0000 | Time Elapsed: 7.63s\n",
      "Epoch: 1 | Step: 122 | Avg Loss: 0.0000 | Time Elapsed: 7.85s\n",
      "Epoch: 1 | Step: 123 | Avg Loss: 0.0000 | Time Elapsed: 7.74s\n",
      "Epoch: 1 | Step: 124 | Avg Loss: 0.0000 | Time Elapsed: 4.58s\n",
      "Epoch 1 finished with loss: 0.0000\n"
     ]
    }
   ],
   "source": [
    "optimizer = Adam(model.parameters(), lr=0.001)\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "for e in range(2):\n",
    "    loss = train(e, trainload, model, device, optimizer)\n",
    "    print(f\"Epoch {e} finished with loss: {loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def evaluate_model(model, test_loader, device):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    all_labels = []\n",
    "    all_logits = []\n",
    "\n",
    "    with torch.no_grad():  # No need to track gradients during evaluation\n",
    "        for features, labels in test_loader:\n",
    "            features = features.to(device)\n",
    "            labels = labels.to(device)\n",
    "            logits = model(features)  # Model outputs logits\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_logits.extend(logits.cpu().numpy())\n",
    "\n",
    "    # Convert logits to probabilities for AUC calculation\n",
    "    probabilities = torch.sigmoid(torch.tensor(all_logits)).numpy()\n",
    "\n",
    "    # Flatten arrays (important if labels are sequences)\n",
    "    all_labels_flat = np.concatenate(all_labels)\n",
    "    probabilities_flat = np.concatenate(probabilities)\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(all_labels_flat, np.round(probabilities_flat))\n",
    "    precision = precision_score(all_labels_flat, np.round(probabilities_flat))\n",
    "    recall = recall_score(all_labels_flat, np.round(probabilities_flat))\n",
    "    f1 = f1_score(all_labels_flat, np.round(probabilities_flat))\n",
    "    auc = roc_auc_score(all_labels_flat, probabilities_flat)\n",
    "\n",
    "    return accuracy, precision, recall, f1, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_5092\\3392295728.py:19: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:248.)\n",
      "  probabilities = torch.sigmoid(torch.tensor(all_logits)).numpy()\n",
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Only one class present in y_true. ROC AUC score is not defined in that case.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m accuracy, precision, recall, f1, auc \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtestload\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccuracy\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mPrecision: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprecision\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mRecall: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrecall\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mF1 Score: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mf1\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mAUC-ROC: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mauc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[25], line 30\u001b[0m, in \u001b[0;36mevaluate_model\u001b[1;34m(model, test_loader, device)\u001b[0m\n\u001b[0;32m     28\u001b[0m recall \u001b[38;5;241m=\u001b[39m recall_score(all_labels_flat, np\u001b[38;5;241m.\u001b[39mround(probabilities_flat))\n\u001b[0;32m     29\u001b[0m f1 \u001b[38;5;241m=\u001b[39m f1_score(all_labels_flat, np\u001b[38;5;241m.\u001b[39mround(probabilities_flat))\n\u001b[1;32m---> 30\u001b[0m auc \u001b[38;5;241m=\u001b[39m \u001b[43mroc_auc_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_labels_flat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprobabilities_flat\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m accuracy, precision, recall, f1, auc\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:572\u001b[0m, in \u001b[0;36mroc_auc_score\u001b[1;34m(y_true, y_score, average, sample_weight, max_fpr, multi_class, labels)\u001b[0m\n\u001b[0;32m    570\u001b[0m     labels \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(y_true)\n\u001b[0;32m    571\u001b[0m     y_true \u001b[38;5;241m=\u001b[39m label_binarize(y_true, classes\u001b[38;5;241m=\u001b[39mlabels)[:, \u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m--> 572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_average_binary_score\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    573\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_binary_roc_auc_score\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_fpr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_fpr\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    574\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    575\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    576\u001b[0m \u001b[43m        \u001b[49m\u001b[43maverage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    577\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    578\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    579\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# multilabel-indicator\u001b[39;00m\n\u001b[0;32m    580\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _average_binary_score(\n\u001b[0;32m    581\u001b[0m         partial(_binary_roc_auc_score, max_fpr\u001b[38;5;241m=\u001b[39mmax_fpr),\n\u001b[0;32m    582\u001b[0m         y_true,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    585\u001b[0m         sample_weight\u001b[38;5;241m=\u001b[39msample_weight,\n\u001b[0;32m    586\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_base.py:75\u001b[0m, in \u001b[0;36m_average_binary_score\u001b[1;34m(binary_metric, y_true, y_score, average, sample_weight)\u001b[0m\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m format is not supported\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(y_type))\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 75\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbinary_metric\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_score\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     77\u001b[0m check_consistent_length(y_true, y_score, sample_weight)\n\u001b[0;32m     78\u001b[0m y_true \u001b[38;5;241m=\u001b[39m check_array(y_true)\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:339\u001b[0m, in \u001b[0;36m_binary_roc_auc_score\u001b[1;34m(y_true, y_score, sample_weight, max_fpr)\u001b[0m\n\u001b[0;32m    337\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Binary roc auc score.\"\"\"\u001b[39;00m\n\u001b[0;32m    338\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(np\u001b[38;5;241m.\u001b[39munique(y_true)) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m--> 339\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    340\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOnly one class present in y_true. ROC AUC score \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    341\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis not defined in that case.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    342\u001b[0m     )\n\u001b[0;32m    344\u001b[0m fpr, tpr, _ \u001b[38;5;241m=\u001b[39m roc_curve(y_true, y_score, sample_weight\u001b[38;5;241m=\u001b[39msample_weight)\n\u001b[0;32m    345\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m max_fpr \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m max_fpr \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "\u001b[1;31mValueError\u001b[0m: Only one class present in y_true. ROC AUC score is not defined in that case."
     ]
    }
   ],
   "source": [
    "accuracy, precision, recall, f1, auc = evaluate_model(model, testload, device)\n",
    "print(f\"Accuracy: {accuracy}\\nPrecision: {precision}\\nRecall: {recall}\\nF1 Score: {f1}\\nAUC-ROC: {auc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
